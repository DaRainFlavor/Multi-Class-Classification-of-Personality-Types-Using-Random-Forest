{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Class Classification of 16 Personality Types (MBTI)\n",
                "## Comprehensive ML Techniques Comparison\n",
                "\n",
                "This notebook compares four traditional machine learning techniques for classifying personality types:\n",
                "1. **Gradient Boosting (XGBoost)**\n",
                "2. **Logistic Regression**\n",
                "3. **Linear Discriminant Analysis (LDA)**\n",
                "4. **Random Forest**\n",
                "\n",
                "All models use:\n",
                "- **Same data splits**: 70/15/15 (Train/Validation/Test)\n",
                "- **Same random seed**: 42 (for reproducibility)\n",
                "- **Same dataset**: 16P_eda_cleaned.csv\n",
                "\n",
                "**Note**: Results may vary slightly between different machines due to hardware-dependent threading behavior."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Step 1: Data Gathering and Exploratory Data Analysis (EDA)\n",
                "\n",
                "This section performs comprehensive data analysis based on the `data_gathering_eda.py` script to answer:\n",
                "1. What is the source and size of your dataset?\n",
                "2. What data quality issues did you encounter?\n",
                "3. What preprocessing steps did you apply?\n",
                "4. How did you handle missing values and outliers?\n",
                "5. What insights did your exploratory analysis reveal?\n",
                "\n",
                "**Dataset Source:**\n",
                "> A. Mehta, \"60k Responses of 16 Personalities Test (MBT),\" Kaggle. \n",
                "> Available: https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt/data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries for EDA\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style for visualizations\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"DATA GATHERING AND EXPLORATORY DATA ANALYSIS\")\n",
                "print(\"16 Personalities (MBTI) Dataset\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Data Source and Size"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the raw dataset\n",
                "df_raw = pd.read_csv('16P.csv', encoding='cp1252')\n",
                "\n",
                "print(f\"\"\"\n",
                "Dataset Source:\n",
                "---------------\n",
                "Title: 60k Responses of 16 Personalities Test (MBT)\n",
                "Author: Anshul Mehta\n",
                "Platform: Kaggle\n",
                "URL: https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt/data\n",
                "\n",
                "Dataset Size:\n",
                "-------------\n",
                "‚Ä¢ Number of Rows (Responses): {df_raw.shape[0]:,}\n",
                "‚Ä¢ Number of Columns (Features): {df_raw.shape[1]}\n",
                "‚Ä¢ Memory Usage: {df_raw.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\n",
                "\n",
                "Dataset Structure:\n",
                "------------------\n",
                "‚Ä¢ 1 Response ID column (identifier)\n",
                "‚Ä¢ 60 Survey Question columns (features)\n",
                "‚Ä¢ 1 Personality Type column (target variable)\n",
                "\"\"\")\n",
                "\n",
                "# Display sample data\n",
                "print(\"First 5 Rows (Sample):\")\n",
                "df_raw.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Data Quality Issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"DATA QUALITY ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 2.1 Check Missing Values\n",
                "print(\"\\n2.1 Missing Values Analysis\")\n",
                "print(\"-\"*40)\n",
                "missing_values = df_raw.isnull().sum()\n",
                "missing_count = missing_values.sum()\n",
                "if missing_count == 0:\n",
                "    print(\"‚úì No missing values found in the dataset.\")\n",
                "else:\n",
                "    print(f\"‚úó Found {missing_count} missing values:\")\n",
                "    print(missing_values[missing_values > 0])\n",
                "\n",
                "# 2.2 Check Duplicate Rows\n",
                "print(\"\\n2.2 Duplicate Rows Analysis\")\n",
                "print(\"-\"*40)\n",
                "duplicate_count = df_raw.duplicated().sum()\n",
                "if duplicate_count == 0:\n",
                "    print(\"‚úì No duplicate rows found in the dataset.\")\n",
                "else:\n",
                "    print(f\"‚úó Found {duplicate_count} duplicate rows.\")\n",
                "\n",
                "# 2.3 Check Duplicate Response IDs\n",
                "print(\"\\n2.3 Duplicate Response IDs\")\n",
                "print(\"-\"*40)\n",
                "if 'Response Id' in df_raw.columns:\n",
                "    duplicate_ids = df_raw['Response Id'].duplicated().sum()\n",
                "    if duplicate_ids == 0:\n",
                "        print(\"‚úì All Response IDs are unique.\")\n",
                "    else:\n",
                "        print(f\"‚úó Found {duplicate_ids} duplicate Response IDs.\")\n",
                "\n",
                "# 2.4 Check Target Variable\n",
                "print(\"\\n2.4 Target Variable (Personality Types) Validation\")\n",
                "print(\"-\"*40)\n",
                "valid_types = ['ESTJ', 'ENTJ', 'ESFJ', 'ENFJ', 'ISTJ', 'ISFJ', 'INTJ', 'INFJ',\n",
                "               'ESTP', 'ESFP', 'ENTP', 'ENFP', 'ISTP', 'ISFP', 'INTP', 'INFP']\n",
                "unique_personalities = df_raw['Personality'].unique()\n",
                "invalid_types = [p for p in unique_personalities if p not in valid_types]\n",
                "\n",
                "print(f\"Expected 16 personality types: {len(valid_types)}\")\n",
                "print(f\"Found unique values: {len(unique_personalities)}\")\n",
                "if len(invalid_types) == 0:\n",
                "    print(\"‚úì All personality types are valid MBTI types.\")\n",
                "else:\n",
                "    print(f\"‚úó Found invalid personality types: {invalid_types}\")\n",
                "\n",
                "# Summary\n",
                "print(\"\\n\" + \"-\"*40)\n",
                "print(\"DATA QUALITY SUMMARY:\")\n",
                "print(\"-\"*40)\n",
                "print(\"‚úì The dataset is of HIGH QUALITY with no significant issues!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Exploratory Data Analysis Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean data for visualizations (remove Response Id)\n",
                "df_eda = df_raw.copy()\n",
                "if 'Response Id' in df_eda.columns:\n",
                "    df_eda = df_eda.drop(columns=['Response Id'])\n",
                "\n",
                "# Figure 1: Personality Type Distribution\n",
                "print(\"Figure 1: Personality Type Distribution\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "personality_counts = df_eda['Personality'].value_counts()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "colors = sns.color_palette(\"husl\", 16)\n",
                "bars = ax.bar(personality_counts.index, personality_counts.values, color=colors)\n",
                "ax.set_xlabel('Personality Type', fontsize=12)\n",
                "ax.set_ylabel('Count', fontsize=12)\n",
                "ax.set_title('Distribution of 16 Personality Types', fontsize=14, fontweight='bold')\n",
                "ax.tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Add value labels on bars\n",
                "for bar, count in zip(bars, personality_counts.values):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
                "            f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print statistics\n",
                "print(f\"\\nClass Distribution Statistics:\")\n",
                "print(f\"Most common type: {personality_counts.idxmax()} ({personality_counts.max():,} samples)\")\n",
                "print(f\"Least common type: {personality_counts.idxmin()} ({personality_counts.min():,} samples)\")\n",
                "imbalance_ratio = personality_counts.max() / personality_counts.min()\n",
                "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 2: MBTI Dimensions Pie Charts\n",
                "print(\"Figure 2: MBTI Dimensions Distribution\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "\n",
                "# Calculate dimensions\n",
                "e_count = sum(1 for p in df_eda['Personality'] if p[0] == 'E')\n",
                "i_count = sum(1 for p in df_eda['Personality'] if p[0] == 'I')\n",
                "s_count = sum(1 for p in df_eda['Personality'] if p[1] == 'S')\n",
                "n_count = sum(1 for p in df_eda['Personality'] if p[1] == 'N')\n",
                "t_count = sum(1 for p in df_eda['Personality'] if p[2] == 'T')\n",
                "f_count = sum(1 for p in df_eda['Personality'] if p[2] == 'F')\n",
                "j_count = sum(1 for p in df_eda['Personality'] if p[3] == 'J')\n",
                "p_count = sum(1 for p in df_eda['Personality'] if p[3] == 'P')\n",
                "\n",
                "dimensions = [\n",
                "    (['Extrovert (E)', 'Introvert (I)'], [e_count, i_count], 'Energy: E vs I'),\n",
                "    (['Sensing (S)', 'Intuition (N)'], [s_count, n_count], 'Information: S vs N'),\n",
                "    (['Thinking (T)', 'Feeling (F)'], [t_count, f_count], 'Decisions: T vs F'),\n",
                "    (['Judging (J)', 'Perceiving (P)'], [j_count, p_count], 'Lifestyle: J vs P')\n",
                "]\n",
                "\n",
                "colors_pair = ['#FF6B6B', '#4ECDC4']\n",
                "for ax, (labels, sizes, title) in zip(axes.flat, dimensions):\n",
                "    wedges, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.1f%%', \n",
                "                                       colors=colors_pair, startangle=90)\n",
                "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.suptitle('MBTI Dimensions Distribution', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print dimension statistics\n",
                "print(f\"\\nDimension Distributions:\")\n",
                "print(f\"  Extrovert (E) vs Introvert (I): {e_count:,} vs {i_count:,}\")\n",
                "print(f\"  Sensing (S) vs Intuition (N):   {s_count:,} vs {n_count:,}\")\n",
                "print(f\"  Thinking (T) vs Feeling (F):    {t_count:,} vs {f_count:,}\")\n",
                "print(f\"  Judging (J) vs Perceiving (P):  {j_count:,} vs {p_count:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 3: Response Value Distribution Heatmap\n",
                "print(\"Figure 3: Response Value Distribution Across Questions\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "feature_cols = [col for col in df_eda.columns if col != 'Personality']\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 8))\n",
                "\n",
                "# Get first 15 feature columns for visibility\n",
                "sample_features = feature_cols[:15]\n",
                "short_names = [col[:30] + \"...\" if len(col) > 30 else col for col in sample_features]\n",
                "\n",
                "# Create distribution matrix\n",
                "response_values = [-3, -2, -1, 0, 1, 2, 3]\n",
                "dist_matrix = np.zeros((len(sample_features), len(response_values)))\n",
                "\n",
                "for i, col in enumerate(sample_features):\n",
                "    for j, val in enumerate(response_values):\n",
                "        dist_matrix[i, j] = (df_eda[col] == val).sum()\n",
                "\n",
                "# Normalize to percentages\n",
                "dist_matrix_pct = dist_matrix / dist_matrix.sum(axis=1, keepdims=True) * 100\n",
                "\n",
                "sns.heatmap(dist_matrix_pct, annot=True, fmt='.1f', cmap='YlOrRd',\n",
                "            xticklabels=response_values, yticklabels=short_names, ax=ax)\n",
                "ax.set_xlabel('Response Value', fontsize=12)\n",
                "ax.set_ylabel('Survey Question', fontsize=12)\n",
                "ax.set_title('Response Distribution Across Questions (%)\\n(Showing first 15 questions)', \n",
                "             fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 4: Feature Correlation Heatmap\n",
                "print(\"Figure 4: Feature Correlation Heatmap\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 12))\n",
                "\n",
                "# Select a subset of features for correlation (every 4th question)\n",
                "sample_features_corr = feature_cols[::4][:15]\n",
                "short_names_corr = [col[:25] + \"...\" if len(col) > 25 else col for col in sample_features_corr]\n",
                "\n",
                "corr_matrix = df_eda[sample_features_corr].corr()\n",
                "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
                "\n",
                "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
                "            center=0, square=True, linewidths=0.5,\n",
                "            xticklabels=short_names_corr, yticklabels=short_names_corr, ax=ax)\n",
                "ax.set_title('Feature Correlation Heatmap\\n(Sampled Questions)', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 5: Response Distribution Box Plots\n",
                "print(\"Figure 5: Response Distribution Box Plots\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 6))\n",
                "\n",
                "# Select first 20 features\n",
                "sample_features_box = feature_cols[:20]\n",
                "df_melt = df_eda[sample_features_box].melt(var_name='Question', value_name='Response')\n",
                "df_melt['Question'] = df_melt['Question'].apply(lambda x: x[:20] + \"...\" if len(x) > 20 else x)\n",
                "\n",
                "sns.boxplot(data=df_melt, x='Question', y='Response', ax=ax, palette='Set3')\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=8)\n",
                "ax.set_xlabel('Survey Question', fontsize=12)\n",
                "ax.set_ylabel('Response Value', fontsize=12)\n",
                "ax.set_title('Response Distribution Across Questions (Box Plots)\\n(Showing first 20 questions)', \n",
                "             fontsize=14, fontweight='bold')\n",
                "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.4 Key EDA Insights\n",
                "\n",
                "### Dataset Characteristics\n",
                "- Large-scale dataset with ~60,000 survey responses\n",
                "- 60 psychological assessment questions using 7-point Likert scale (-3 to 3)\n",
                "- Covers all 16 MBTI personality types\n",
                "\n",
                "### Data Quality\n",
                "- ‚úì Excellent data quality - no missing values detected\n",
                "- ‚úì No duplicate entries or invalid personality types\n",
                "- ‚úì All responses within expected value range (-3 to 3)\n",
                "\n",
                "### Class Distribution Insights\n",
                "- Some class imbalance exists between personality types (natural distribution)\n",
                "- Stratified sampling recommended during model training\n",
                "\n",
                "### Preprocessing Applied\n",
                "1. Removed 'Response Id' column (identifier not useful for prediction)\n",
                "2. Verified all feature columns are numeric\n",
                "3. No missing value imputation needed\n",
                "\n",
                "### Conclusions\n",
                "The dataset is well-prepared for multi-class classification using machine learning techniques."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Setup and Data Loading"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ‚ö†Ô∏è IMPORTANT: Run this cell first in Google Colab, then restart runtime!\n",
                "Skip this cell if running locally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# REPRODUCIBILITY SETUP (Run in Colab only)\n",
                "# ========================================\n",
                "# Install exact library versions matching local environment\n",
                "# After running this cell, restart the runtime: Runtime -> Restart runtime\n",
                "\n",
                "!pip install numpy==2.3.3 pandas==2.3.2 scikit-learn==1.7.2 xgboost==3.1.3 -q\n",
                "\n",
                "print(\"‚úì Libraries installed. Please restart runtime now!\")\n",
                "print(\"Go to: Runtime -> Restart runtime, then run all cells below.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, \n",
                "    classification_report, \n",
                "    confusion_matrix,\n",
                "    f1_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    top_k_accuracy_score\n",
                ")\n",
                "from xgboost import XGBClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "# Print library versions for verification\n",
                "print(f\"numpy: {np.__version__}\")\n",
                "print(f\"pandas: {pd.__version__}\")\n",
                "import sklearn; print(f\"scikit-learn: {sklearn.__version__}\")\n",
                "import xgboost; print(f\"xgboost: {xgboost.__version__}\")\n",
                "print(\"\\n‚úì All libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the cleaned data from EDA section (df_eda already prepared above)\n",
                "# Rename to df for consistency with the rest of the notebook\n",
                "df = df_eda.copy()\n",
                "\n",
                "print(f\"Dataset loaded from EDA: {len(df):,} rows and {len(df.columns)} columns\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X = df.drop(columns=['Personality'])\n",
                "y = df['Personality']\n",
                "\n",
                "feature_columns = X.columns.tolist()\n",
                "print(f\"Features: {len(feature_columns)} survey questions\")\n",
                "print(f\"Target: 16 MBTI personality types\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(y.value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode target labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "class_names = label_encoder.classes_\n",
                "print(f\"Encoded {len(class_names)} classes: {', '.join(class_names)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data (70% Train, 15% Validation, 15% Test)\n",
                "TEST_SIZE = 0.15      # 15% for test\n",
                "VAL_SIZE = 0.176      # 15% of remaining 85% ‚âà 15% of total\n",
                "\n",
                "# First split: 85% train+val, 15% test\n",
                "X_temp, X_test, y_temp, y_test = train_test_split(\n",
                "    X, y_encoded, \n",
                "    test_size=TEST_SIZE, \n",
                "    random_state=RANDOM_STATE, \n",
                "    stratify=y_encoded\n",
                ")\n",
                "\n",
                "# Second split: 70% train, 15% val (of total)\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_temp, y_temp, \n",
                "    test_size=VAL_SIZE, \n",
                "    random_state=RANDOM_STATE, \n",
                "    stratify=y_temp\n",
                ")\n",
                "\n",
                "print(f\"Training set:   {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
                "print(f\"Validation set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
                "print(f\"Test set:       {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature scaling (for Logistic Regression and LDA)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"‚úì Features standardized to zero mean and unit variance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Model 1: Gradient Boosting (XGBoost)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"TRAINING GRADIENT BOOSTING (XGBoost) CLASSIFIER\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Configure XGBoost (matching local xgboost_classifier.py)\n",
                "xgb_params = {\n",
                "    'n_estimators': 500,\n",
                "    'learning_rate': 0.1,\n",
                "    'max_depth': 6,\n",
                "    'min_child_weight': 1,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'objective': 'multi:softprob',\n",
                "    'num_class': 16,\n",
                "    'random_state': RANDOM_STATE,\n",
                "    'n_jobs': -1,\n",
                "    'verbosity': 0\n",
                "}\n",
                "\n",
                "print(\"\\nHyperparameters:\")\n",
                "for key, value in xgb_params.items():\n",
                "    if key not in ['verbosity', 'n_jobs']:\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "# Train model\n",
                "xgb_model = XGBClassifier(**xgb_params, early_stopping_rounds=15)\n",
                "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Best iteration: {xgb_model.best_iteration}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "xgb_train_pred = xgb_model.predict(X_train)\n",
                "xgb_val_pred = xgb_model.predict(X_val)\n",
                "xgb_test_pred = xgb_model.predict(X_test)\n",
                "xgb_test_proba = xgb_model.predict_proba(X_test)\n",
                "\n",
                "# Calculate metrics\n",
                "xgb_train_acc = accuracy_score(y_train, xgb_train_pred)\n",
                "xgb_val_acc = accuracy_score(y_val, xgb_val_pred)\n",
                "xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
                "xgb_f1 = f1_score(y_test, xgb_test_pred, average='macro')\n",
                "xgb_precision = precision_score(y_test, xgb_test_pred, average='macro')\n",
                "xgb_recall = recall_score(y_test, xgb_test_pred, average='macro')\n",
                "xgb_top3 = top_k_accuracy_score(y_test, xgb_test_proba, k=3)\n",
                "\n",
                "print(\"GRADIENT BOOSTING RESULTS\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"Training Accuracy:   {xgb_train_acc:.4f} ({xgb_train_acc*100:.2f}%)\")\n",
                "print(f\"Validation Accuracy: {xgb_val_acc:.4f} ({xgb_val_acc*100:.2f}%)\")\n",
                "print(f\"Test Accuracy:       {xgb_test_acc:.4f} ({xgb_test_acc*100:.2f}%)\")\n",
                "print(f\"\\nTop-3 Accuracy:      {xgb_top3:.4f} ({xgb_top3*100:.2f}%)\")\n",
                "print(f\"Macro Precision:     {xgb_precision:.4f}\")\n",
                "print(f\"Macro Recall:        {xgb_recall:.4f}\")\n",
                "print(f\"Macro F1-Score:      {xgb_f1:.4f}\")\n",
                "\n",
                "xgb_cm = confusion_matrix(y_test, xgb_test_pred)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Model 2: Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"TRAINING LOGISTIC REGRESSION CLASSIFIER\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Configure Logistic Regression\n",
                "lr_params = {\n",
                "    'multi_class': 'multinomial',\n",
                "    'solver': 'lbfgs',\n",
                "    'max_iter': 1000,\n",
                "    'C': 1.0,\n",
                "    'random_state': RANDOM_STATE,\n",
                "    'n_jobs': -1,\n",
                "    'verbose': 0\n",
                "}\n",
                "\n",
                "print(\"\\nHyperparameters:\")\n",
                "for key, value in lr_params.items():\n",
                "    if key not in ['verbose', 'n_jobs']:\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "# Train model (uses scaled data)\n",
                "lr_model = LogisticRegression(**lr_params)\n",
                "lr_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Iterations: {lr_model.n_iter_[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "lr_train_pred = lr_model.predict(X_train_scaled)\n",
                "lr_val_pred = lr_model.predict(X_val_scaled)\n",
                "lr_test_pred = lr_model.predict(X_test_scaled)\n",
                "lr_test_proba = lr_model.predict_proba(X_test_scaled)\n",
                "\n",
                "# Calculate metrics\n",
                "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
                "lr_val_acc = accuracy_score(y_val, lr_val_pred)\n",
                "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
                "lr_f1 = f1_score(y_test, lr_test_pred, average='macro')\n",
                "lr_precision = precision_score(y_test, lr_test_pred, average='macro')\n",
                "lr_recall = recall_score(y_test, lr_test_pred, average='macro')\n",
                "lr_top3 = top_k_accuracy_score(y_test, lr_test_proba, k=3)\n",
                "\n",
                "print(\"LOGISTIC REGRESSION RESULTS\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"Training Accuracy:   {lr_train_acc:.4f} ({lr_train_acc*100:.2f}%)\")\n",
                "print(f\"Validation Accuracy: {lr_val_acc:.4f} ({lr_val_acc*100:.2f}%)\")\n",
                "print(f\"Test Accuracy:       {lr_test_acc:.4f} ({lr_test_acc*100:.2f}%)\")\n",
                "print(f\"\\nTop-3 Accuracy:      {lr_top3:.4f} ({lr_top3*100:.2f}%)\")\n",
                "print(f\"Macro Precision:     {lr_precision:.4f}\")\n",
                "print(f\"Macro Recall:        {lr_recall:.4f}\")\n",
                "print(f\"Macro F1-Score:      {lr_f1:.4f}\")\n",
                "\n",
                "lr_cm = confusion_matrix(y_test, lr_test_pred)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Model 3: Linear Discriminant Analysis (LDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"TRAINING LINEAR DISCRIMINANT ANALYSIS (LDA) CLASSIFIER\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Configure LDA\n",
                "lda_params = {\n",
                "    'solver': 'svd',\n",
                "    'n_components': None,\n",
                "    'store_covariance': False,\n",
                "    'tol': 1e-4\n",
                "}\n",
                "\n",
                "print(\"\\nHyperparameters:\")\n",
                "for key, value in lda_params.items():\n",
                "    print(f\"  {key}: {value}\")\n",
                "\n",
                "# Train model (uses scaled data)\n",
                "lda_model = LinearDiscriminantAnalysis(**lda_params)\n",
                "lda_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "n_components = lda_model.scalings_.shape[1]\n",
                "print(f\"\\n‚úì Training complete! Discriminant components: {n_components}\")\n",
                "print(f\"Explained variance ratio (first 5): {lda_model.explained_variance_ratio_[:5].round(4).tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "lda_train_pred = lda_model.predict(X_train_scaled)\n",
                "lda_val_pred = lda_model.predict(X_val_scaled)\n",
                "lda_test_pred = lda_model.predict(X_test_scaled)\n",
                "lda_test_proba = lda_model.predict_proba(X_test_scaled)\n",
                "\n",
                "# Calculate metrics\n",
                "lda_train_acc = accuracy_score(y_train, lda_train_pred)\n",
                "lda_val_acc = accuracy_score(y_val, lda_val_pred)\n",
                "lda_test_acc = accuracy_score(y_test, lda_test_pred)\n",
                "lda_f1 = f1_score(y_test, lda_test_pred, average='macro')\n",
                "lda_precision = precision_score(y_test, lda_test_pred, average='macro')\n",
                "lda_recall = recall_score(y_test, lda_test_pred, average='macro')\n",
                "lda_top3 = top_k_accuracy_score(y_test, lda_test_proba, k=3)\n",
                "\n",
                "print(\"LINEAR DISCRIMINANT ANALYSIS RESULTS\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"Training Accuracy:   {lda_train_acc:.4f} ({lda_train_acc*100:.2f}%)\")\n",
                "print(f\"Validation Accuracy: {lda_val_acc:.4f} ({lda_val_acc*100:.2f}%)\")\n",
                "print(f\"Test Accuracy:       {lda_test_acc:.4f} ({lda_test_acc*100:.2f}%)\")\n",
                "print(f\"\\nTop-3 Accuracy:      {lda_top3:.4f} ({lda_top3*100:.2f}%)\")\n",
                "print(f\"Macro Precision:     {lda_precision:.4f}\")\n",
                "print(f\"Macro Recall:        {lda_recall:.4f}\")\n",
                "print(f\"Macro F1-Score:      {lda_f1:.4f}\")\n",
                "\n",
                "lda_cm = confusion_matrix(y_test, lda_test_pred)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Model 4: Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"TRAINING RANDOM FOREST CLASSIFIER\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Configure Random Forest\n",
                "rf_params = {\n",
                "    'n_estimators': 100,\n",
                "    'max_depth': 20,\n",
                "    'min_samples_split': 5,\n",
                "    'min_samples_leaf': 2,\n",
                "    'max_features': 'sqrt',\n",
                "    'random_state': RANDOM_STATE,\n",
                "    'n_jobs': -1,\n",
                "    'verbose': 0\n",
                "}\n",
                "\n",
                "print(\"\\nHyperparameters:\")\n",
                "for key, value in rf_params.items():\n",
                "    if key not in ['verbose', 'n_jobs']:\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "# Train model\n",
                "rf_model = RandomForestClassifier(**rf_params)\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "print(f\"\\n‚úì Training complete! Number of trees: {len(rf_model.estimators_)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "rf_train_pred = rf_model.predict(X_train)\n",
                "rf_val_pred = rf_model.predict(X_val)\n",
                "rf_test_pred = rf_model.predict(X_test)\n",
                "rf_test_proba = rf_model.predict_proba(X_test)\n",
                "\n",
                "# Calculate metrics\n",
                "rf_train_acc = accuracy_score(y_train, rf_train_pred)\n",
                "rf_val_acc = accuracy_score(y_val, rf_val_pred)\n",
                "rf_test_acc = accuracy_score(y_test, rf_test_pred)\n",
                "rf_f1 = f1_score(y_test, rf_test_pred, average='macro')\n",
                "rf_precision = precision_score(y_test, rf_test_pred, average='macro')\n",
                "rf_recall = recall_score(y_test, rf_test_pred, average='macro')\n",
                "rf_top3 = top_k_accuracy_score(y_test, rf_test_proba, k=3)\n",
                "\n",
                "print(\"RANDOM FOREST RESULTS\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"Training Accuracy:   {rf_train_acc:.4f} ({rf_train_acc*100:.2f}%)\")\n",
                "print(f\"Validation Accuracy: {rf_val_acc:.4f} ({rf_val_acc*100:.2f}%)\")\n",
                "print(f\"Test Accuracy:       {rf_test_acc:.4f} ({rf_test_acc*100:.2f}%)\")\n",
                "print(f\"\\nTop-3 Accuracy:      {rf_top3:.4f} ({rf_top3*100:.2f}%)\")\n",
                "print(f\"Macro Precision:     {rf_precision:.4f}\")\n",
                "print(f\"Macro Recall:        {rf_recall:.4f}\")\n",
                "print(f\"Macro F1-Score:      {rf_f1:.4f}\")\n",
                "\n",
                "rf_cm = confusion_matrix(y_test, rf_test_pred)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üìä MODEL COMPARISONS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Overall Performance Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison dataframe\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Model': ['Gradient Boosting', 'Logistic Regression', 'LDA', 'Random Forest'],\n",
                "    'Train Accuracy': [xgb_train_acc, lr_train_acc, lda_train_acc, rf_train_acc],\n",
                "    'Val Accuracy': [xgb_val_acc, lr_val_acc, lda_val_acc, rf_val_acc],\n",
                "    'Test Accuracy': [xgb_test_acc, lr_test_acc, lda_test_acc, rf_test_acc],\n",
                "    'Top-3 Accuracy': [xgb_top3, lr_top3, lda_top3, rf_top3],\n",
                "    'Macro Precision': [xgb_precision, lr_precision, lda_precision, rf_precision],\n",
                "    'Macro Recall': [xgb_recall, lr_recall, lda_recall, rf_recall],\n",
                "    'Macro F1-Score': [xgb_f1, lr_f1, lda_f1, rf_f1]\n",
                "})\n",
                "\n",
                "# Add overfitting gap\n",
                "comparison_df['Overfit Gap'] = comparison_df['Train Accuracy'] - comparison_df['Test Accuracy']\n",
                "\n",
                "# Display with formatting\n",
                "print(\"=\" * 120)\n",
                "print(\"COMPREHENSIVE MODEL COMPARISON TABLE\")\n",
                "print(\"=\" * 120)\n",
                "print(comparison_df.to_string(index=False))\n",
                "print(\"=\" * 120)\n",
                "\n",
                "# Find best performers\n",
                "best_test_acc = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']\n",
                "best_f1 = comparison_df.loc[comparison_df['Macro F1-Score'].idxmax(), 'Model']\n",
                "best_top3 = comparison_df.loc[comparison_df['Top-3 Accuracy'].idxmax(), 'Model']\n",
                "least_overfit = comparison_df.loc[comparison_df['Overfit Gap'].idxmin(), 'Model']\n",
                "\n",
                "print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
                "print(f\"  ‚Ä¢ Highest Test Accuracy: {best_test_acc}\")\n",
                "print(f\"  ‚Ä¢ Highest Macro F1-Score: {best_f1}\")\n",
                "print(f\"  ‚Ä¢ Highest Top-3 Accuracy: {best_top3}\")\n",
                "print(f\"  ‚Ä¢ Least Overfitting: {least_overfit}\")\n",
                "\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Visual Comparison: Test Accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bar chart comparing test accuracies\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "colors = ['#1f77b4', '#2ca02c', '#9467bd', '#ff7f0e']\n",
                "bars = ax.bar(comparison_df['Model'], comparison_df['Test Accuracy'], color=colors, alpha=0.8)\n",
                "\n",
                "# Add value labels on bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "            f'{height:.4f}\\n({height*100:.2f}%)',\n",
                "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Model Comparison: Test Set Accuracy', fontsize=14, fontweight='bold')\n",
                "ax.set_ylim(0, 1)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "plt.xticks(rotation=15)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visual Comparison: All Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multi-metric comparison\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "fig.suptitle('Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold')\n",
                "\n",
                "metrics = [\n",
                "    ('Test Accuracy', 'Test Accuracy'),\n",
                "    ('Macro F1-Score', 'Macro F1-Score'),\n",
                "    ('Top-3 Accuracy', 'Top-3 Accuracy'),\n",
                "    ('Overfit Gap', 'Overfitting Gap (lower is better)')\n",
                "]\n",
                "\n",
                "for idx, (metric, title) in enumerate(metrics):\n",
                "    row, col = idx // 2, idx % 2\n",
                "    ax = axes[row, col]\n",
                "    \n",
                "    # Use different color for overfitting (lower is better)\n",
                "    if metric == 'Overfit Gap':\n",
                "        color_palette = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']  # Red to blue\n",
                "    else:\n",
                "        color_palette = colors\n",
                "    \n",
                "    bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=color_palette, alpha=0.8)\n",
                "    \n",
                "    # Add value labels\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "                f'{height:.4f}',\n",
                "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
                "    \n",
                "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
                "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
                "    ax.grid(axis='y', alpha=0.3)\n",
                "    ax.tick_params(axis='x', rotation=15)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train vs Test Accuracy (Overfitting Analysis)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Overfitting visualization\n",
                "fig, ax = plt.subplots(figsize=(12, 7))\n",
                "\n",
                "x = np.arange(len(comparison_df))\n",
                "width = 0.35\n",
                "\n",
                "bars1 = ax.bar(x - width/2, comparison_df['Train Accuracy'], width, \n",
                "               label='Train Accuracy', color='steelblue', alpha=0.8)\n",
                "bars2 = ax.bar(x + width/2, comparison_df['Test Accuracy'], width,\n",
                "               label='Test Accuracy', color='coral', alpha=0.8)\n",
                "\n",
                "# Add value labels\n",
                "for bars in [bars1, bars2]:\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "                f'{height:.3f}',\n",
                "                ha='center', va='bottom', fontsize=9)\n",
                "\n",
                "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Train vs Test Accuracy - Overfitting Analysis', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(comparison_df['Model'], rotation=15)\n",
                "ax.legend(fontsize=11)\n",
                "ax.set_ylim(0, 1.1)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüìä OVERFITTING ANALYSIS:\")\n",
                "for idx, row in comparison_df.iterrows():\n",
                "    gap = row['Overfit Gap']\n",
                "    status = \"‚úì Good generalization\" if gap < 0.05 else \"‚ö† Potential overfitting\"\n",
                "    print(f\"  {row['Model']:20s}: Gap = {gap:.4f} ({gap*100:.2f}%) - {status}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confusion Matrix Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot all confusion matrices side by side\n",
                "fig, axes = plt.subplots(2, 2, figsize=(20, 18))\n",
                "fig.suptitle('Confusion Matrix Comparison - All Models', fontsize=16, fontweight='bold', y=0.995)\n",
                "\n",
                "cms = [xgb_cm, lr_cm, lda_cm, rf_cm]\n",
                "model_names = ['Gradient Boosting (XGBoost)', 'Logistic Regression', 'LDA', 'Random Forest']\n",
                "cmaps = ['Blues', 'Greens', 'Purples', 'Oranges']\n",
                "\n",
                "for idx, (cm, name, cmap) in enumerate(zip(cms, model_names, cmaps)):\n",
                "    row, col = idx // 2, idx % 2\n",
                "    ax = axes[row, col]\n",
                "    \n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax,\n",
                "                xticklabels=class_names, yticklabels=class_names,\n",
                "                cbar_kws={'label': 'Count'})\n",
                "    \n",
                "    ax.set_title(f'{name}\\nTest Accuracy: {comparison_df.iloc[idx][\"Test Accuracy\"]:.4f}', \n",
                "                 fontsize=12, fontweight='bold')\n",
                "    ax.set_xlabel('Predicted', fontsize=10)\n",
                "    ax.set_ylabel('Actual', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Per-Class Accuracy Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate per-class accuracy for all models\n",
                "xgb_per_class = xgb_cm.diagonal() / xgb_cm.sum(axis=1)\n",
                "lr_per_class = lr_cm.diagonal() / lr_cm.sum(axis=1)\n",
                "lda_per_class = lda_cm.diagonal() / lda_cm.sum(axis=1)\n",
                "rf_per_class = rf_cm.diagonal() / rf_cm.sum(axis=1)\n",
                "\n",
                "# Create comparison plot\n",
                "fig, ax = plt.subplots(figsize=(16, 8))\n",
                "\n",
                "x = np.arange(len(class_names))\n",
                "width = 0.2\n",
                "\n",
                "ax.bar(x - 1.5*width, xgb_per_class, width, label='Gradient Boosting', color='#1f77b4', alpha=0.8)\n",
                "ax.bar(x - 0.5*width, lr_per_class, width, label='Logistic Regression', color='#2ca02c', alpha=0.8)\n",
                "ax.bar(x + 0.5*width, lda_per_class, width, label='LDA', color='#9467bd', alpha=0.8)\n",
                "ax.bar(x + 1.5*width, rf_per_class, width, label='Random Forest', color='#ff7f0e', alpha=0.8)\n",
                "\n",
                "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
                "ax.set_xlabel('Personality Type', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Per-Class Accuracy Comparison Across All Models', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(class_names, rotation=45)\n",
                "ax.legend(fontsize=11, loc='lower right')\n",
                "ax.set_ylim(0, 1)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Importance Comparison (Top 15 Features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance from each model\n",
                "xgb_importance = pd.DataFrame({\n",
                "    'Feature': feature_columns,\n",
                "    'Importance': xgb_model.feature_importances_\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "lr_importance = pd.DataFrame({\n",
                "    'Feature': feature_columns,\n",
                "    'Importance': np.mean(np.abs(lr_model.coef_), axis=0)\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "lda_importance = pd.DataFrame({\n",
                "    'Feature': feature_columns,\n",
                "    'Importance': np.mean(np.abs(lda_model.coef_), axis=0)\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "rf_importance = pd.DataFrame({\n",
                "    'Feature': feature_columns,\n",
                "    'Importance': rf_model.feature_importances_\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "# Display top 15 features for each model\n",
                "fig, axes = plt.subplots(2, 2, figsize=(18, 16))\n",
                "fig.suptitle('Top 15 Most Important Features - All Models', fontsize=16, fontweight='bold')\n",
                "\n",
                "importances = [xgb_importance, lr_importance, lda_importance, rf_importance]\n",
                "titles = ['Gradient Boosting', 'Logistic Regression', 'LDA', 'Random Forest']\n",
                "colors_imp = ['steelblue', 'forestgreen', 'mediumpurple', 'coral']\n",
                "\n",
                "for idx, (importance_df, title, color) in enumerate(zip(importances, titles, colors_imp)):\n",
                "    row, col = idx // 2, idx % 2\n",
                "    ax = axes[row, col]\n",
                "    \n",
                "    top_15 = importance_df.head(15)\n",
                "    short_names = [f[:35] + \"...\" if len(f) > 35 else f for f in top_15['Feature']]\n",
                "    \n",
                "    ax.barh(range(len(top_15)), top_15['Importance'].values, color=color, alpha=0.8)\n",
                "    ax.set_yticks(range(len(top_15)))\n",
                "    ax.set_yticklabels(short_names, fontsize=9)\n",
                "    ax.invert_yaxis()\n",
                "    ax.set_xlabel('Importance Score', fontsize=10, fontweight='bold')\n",
                "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
                "    ax.grid(axis='x', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Statistical Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 100)\n",
                "print(\"STATISTICAL SUMMARY ACROSS ALL MODELS\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "summary_stats = comparison_df[['Test Accuracy', 'Macro F1-Score', 'Top-3 Accuracy', 'Overfit Gap']].describe()\n",
                "print(summary_stats)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 100)\n",
                "print(\"MODEL RANKINGS\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "# Rank models by different metrics\n",
                "metrics_to_rank = ['Test Accuracy', 'Macro F1-Score', 'Top-3 Accuracy']\n",
                "for metric in metrics_to_rank:\n",
                "    sorted_df = comparison_df.sort_values(metric, ascending=False)\n",
                "    print(f\"\\n{metric}:\")\n",
                "    for rank, (idx, row) in enumerate(sorted_df.iterrows(), 1):\n",
                "        print(f\"  {rank}. {row['Model']:20s} - {row[metric]:.4f}\")\n",
                "\n",
                "# Overfitting ranking (lower is better)\n",
                "sorted_df = comparison_df.sort_values('Overfit Gap', ascending=True)\n",
                "print(f\"\\nLeast Overfitting (Train-Test Gap):\")\n",
                "for rank, (idx, row) in enumerate(sorted_df.iterrows(), 1):\n",
                "    print(f\"  {rank}. {row['Model']:20s} - {row['Overfit Gap']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üéØ Final Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 100)\n",
                "print(\"FINAL RECOMMENDATIONS & KEY INSIGHTS\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "# Find best overall model\n",
                "best_idx = comparison_df['Test Accuracy'].idxmax()\n",
                "best_model = comparison_df.iloc[best_idx]\n",
                "\n",
                "print(f\"\\nüèÜ BEST OVERALL MODEL: {best_model['Model']}\")\n",
                "print(f\"   Test Accuracy: {best_model['Test Accuracy']:.4f} ({best_model['Test Accuracy']*100:.2f}%)\")\n",
                "print(f\"   Macro F1-Score: {best_model['Macro F1-Score']:.4f}\")\n",
                "print(f\"   Top-3 Accuracy: {best_model['Top-3 Accuracy']:.4f} ({best_model['Top-3 Accuracy']*100:.2f}%)\")\n",
                "print(f\"   Overfitting Gap: {best_model['Overfit Gap']:.4f} ({best_model['Overfit Gap']*100:.2f}%)\")\n",
                "\n",
                "print(\"\\nüìä KEY INSIGHTS:\")\n",
                "print(\"\\n1. ACCURACY PERFORMANCE:\")\n",
                "print(f\"   ‚Ä¢ Highest test accuracy: {comparison_df['Test Accuracy'].max():.4f}\")\n",
                "print(f\"   ‚Ä¢ Lowest test accuracy: {comparison_df['Test Accuracy'].min():.4f}\")\n",
                "print(f\"   ‚Ä¢ Range: {(comparison_df['Test Accuracy'].max() - comparison_df['Test Accuracy'].min()):.4f}\")\n",
                "\n",
                "print(\"\\n2. GENERALIZATION:\")\n",
                "overfitting_models = comparison_df[comparison_df['Overfit Gap'] > 0.05]\n",
                "if len(overfitting_models) > 0:\n",
                "    print(f\"   ‚Ä¢ {len(overfitting_models)} model(s) show potential overfitting:\")\n",
                "    for idx, row in overfitting_models.iterrows():\n",
                "        print(f\"     - {row['Model']}: Gap = {row['Overfit Gap']:.4f}\")\n",
                "else:\n",
                "    print(\"   ‚Ä¢ All models generalize well (overfit gap < 5%)\")\n",
                "\n",
                "print(\"\\n3. TOP-3 ACCURACY:\")\n",
                "print(f\"   ‚Ä¢ Best top-3 accuracy: {comparison_df['Top-3 Accuracy'].max():.4f} ({best_top3})\")\n",
                "print(f\"   ‚Ä¢ All models achieve >70% top-3 accuracy\" if comparison_df['Top-3 Accuracy'].min() > 0.7 else \"\")\n",
                "\n",
                "print(\"\\n4. RECOMMENDATIONS:\")\n",
                "print(f\"   ‚Ä¢ For HIGHEST ACCURACY: Use {best_test_acc}\")\n",
                "print(f\"   ‚Ä¢ For BEST BALANCED PERFORMANCE: Use {best_f1}\")\n",
                "print(f\"   ‚Ä¢ For LEAST OVERFITTING: Use {least_overfit}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 100)\n",
                "print(\"ANALYSIS COMPLETE\")\n",
                "print(\"=\" * 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìÅ Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save comparison results to CSV\n",
                "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
                "print(\"‚úì Saved: model_comparison_results.csv\")\n",
                "\n",
                "# Save per-class accuracies\n",
                "per_class_df = pd.DataFrame({\n",
                "    'Personality Type': class_names,\n",
                "    'Gradient Boosting': xgb_per_class,\n",
                "    'Logistic Regression': lr_per_class,\n",
                "    'LDA': lda_per_class,\n",
                "    'Random Forest': rf_per_class\n",
                "})\n",
                "per_class_df.to_csv('per_class_accuracy_comparison.csv', index=False)\n",
                "print(\"‚úì Saved: per_class_accuracy_comparison.csv\")\n",
                "\n",
                "print(\"\\n‚úÖ All results exported successfully!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}